# app.py â€” RAG + PASS/FAIL í•™ìŠµ + Vision ì¶”ì¶œ
# íŒ¨ì¹˜ í¬í•¨: ê°•ì œ JSON, í‚¤ ì •ê·œí™”/ì¼ê´€í™”, ë¹ˆì¹¸ í† í° ì²˜ë¦¬, PASS/FAIL ë©”íƒ€/ì¤‘ë³µë°©ì§€,
# ì—…ë¡œë“œ-ì¦‰ì‹œë¶„ì„(UX), ìŠ¤ìº”PDF ê²½ê³ , í†µí•©íŒì • í”„ë¡¬í”„íŠ¸ ìš°ì„ ìˆœìœ„, ë¡œì»¬ FAIL ê·œì¹™ ê°•í™” (ë‚ ì§œ, ê¸ˆì•¡, í•„ìˆ˜ ì²¨ë¶€)

import streamlit as st
import os, io, json, base64, glob, re, hashlib, unicodedata, string
from datetime import datetime
from typing import List, Dict, Any
from PIL import Image
from openai import OpenAI
from pypdf import PdfReader
import chromadb
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# ==============================
# ì„¤ì •
# ==============================
APP_TITLE = "ğŸ“„ AI ê²°ì¬ ì‚¬ì „ê²€í†  (RAG + PASS/FAIL í•™ìŠµ í†µí•©í˜•)"
# ğŸš¨ í´ë¼ìš°ë“œ í™˜ê²½(Streamlit Cloud)ì—ì„œ íŒŒì¼ ì‹œìŠ¤í…œ ê¶Œí•œ ì˜¤ë¥˜(Read-only)ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´
# ğŸš¨ /tmp ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë””ë ‰í† ë¦¬ëŠ” ì“°ê¸°ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, ì•± ì¬ì‹œì‘ ì‹œ ë°ì´í„°ëŠ” ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
DB_DIR = "/tmp/chroma_db"
DATASET_PATH = "/tmp/pass_fail_dataset.json" # ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œë„ /tmpë¡œ ë³€ê²½
GUIDE_COLLECTION_NAME = "company_guideline"

st.set_page_config(page_title=APP_TITLE, layout="wide")
st.title(APP_TITLE)

# ==============================
# ì´ˆê¸°í™”
# ==============================
# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    # /tmp ê²½ë¡œì— ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„± ì‹œë„
    chroma_client = chromadb.PersistentClient(path=DB_DIR)
except Exception as e:
    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ (ì˜ˆ: í´ë¼ìš°ë“œ í™˜ê²½ì˜ ê¶Œí•œ ë¬¸ì œ), í´ë¼ì´ì–¸íŠ¸ ë¹„í™œì„±í™”
    st.error(f"ChromaDB ì´ˆê¸°í™” ì˜¤ë¥˜ (íŒŒì¼ ì‹œìŠ¤í…œ ê¶Œí•œ ë¬¸ì œ ì˜ˆìƒ): {e}")
    st.warning(f"ChromaDB ê²½ë¡œë¥¼ '{DB_DIR}'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. ë§Œì•½ ì˜¤ë¥˜ê°€ ì§€ì†ëœë‹¤ë©´ ì„ë² ë”©/RAG ê¸°ëŠ¥ì€ ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    chroma_client = None
    
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "test_json" not in st.session_state:
    st.session_state["test_json"] = None
if "local_fail_reasons" not in st.session_state:
    st.session_state["local_fail_reasons"] = []


# ==============================
# ê³µí†µ ìœ í‹¸
# ==============================
def pdf_to_text(file: bytes) -> str:
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFëŠ” ë°”ë¡œ ì¶”ì¶œ, ìŠ¤ìº” PDFë©´ ë¹ˆ ë¬¸ìì—´ ê°€ëŠ¥"""
    try:
        reader = PdfReader(io.BytesIO(file))
        texts = [page.extract_text() or "" for page in reader.pages]
        text = "\n".join(texts)
        if not text.strip():
            st.warning("âš ï¸ PDFì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤ìº” PDFì¼ ìˆ˜ ìˆì–´ìš”.")
        return text
    except Exception as e:
        st.error(f"PDF ì½ê¸° ì˜¤ë¥˜: {e}")
        return ""

def split_text(text: str, chunk_size: int = 1500, overlap: int = 100) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• í•˜ì—¬ ì„ë² ë”© ì²­í¬ë¡œ ë§Œë“¬ (chunk_sizeë¥¼ 1500ìœ¼ë¡œ ìƒí–¥)"""
    text = text.replace("\r", "\n")
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(L, start + chunk_size)
        chunk = text[start:end]
        if chunk.strip():
            chunks.append(chunk)
        
        # ì¤‘ì²©(overlap)ì„ ì ìš©í•˜ì—¬ ë‹¤ìŒ ì‹œì‘ ì§€ì ì„ ê³„ì‚°
        start = end - overlap
        if start < 0: start = 0
        if start >= L: break
    return chunks

def embed_texts(texts: List[str], api_key: str) -> List[List[float]]:
    """OpenAI ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ëª¨ë¸: text-embedding-3-small)"""
    if not texts:
        return []
    try:
        # ëª¨ë¸ì„ text-embedding-3-smallë¡œ ë³€ê²½í•˜ì—¬ ì†ë„ í–¥ìƒ
        embedder = OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key)
        return embedder.embed_documents(texts)
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return []

def pil_to_b64(img: Image.Image) -> str:
    """PIL Image ê°ì²´ë¥¼ base64 ì¸ì½”ë”©ëœ ë¬¸ìì—´ë¡œ ë³€í™˜ (GPT Vision APIìš©)"""
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buf.getvalue()).decode("utf-8")

# ==============================
# í‚¤ ì •ê·œí™” / ë¹ˆì¹¸ íŒì •
# ==============================
ALIASES = {
    # ì²¨ë¶€/ì¦ë¹™ ê´€ë ¨ â†’ attachment_countë¡œ í†µì¼
    "ì²¨ë¶€íŒŒì¼ìˆ˜": "attachment_count",
    "ì²¨ë¶€": "attachment_count",
    "ì¦ë¹™ê°œìˆ˜": "attachment_count",
    "ì²¨ë¶€(ê±´ìˆ˜)": "attachment_count",
    # ê¸°íƒ€ ìì£¼ ë³´ì´ëŠ” í•œê¸€ í‚¤ ì¹˜í™˜ì„ ì—¬ê¸°ì— ê³„ì† ë³´ê°•í•´ë„ ë¨
    "ì²­êµ¬ê¸ˆì•¡": "ê¸ˆì•¡",
    "ì´ê¸ˆì•¡": "ê¸ˆì•¡",
}

def normalize_keys(d: Dict[str, Any]) -> Dict[str, Any]:
    """ë”•ì…”ë„ˆë¦¬ í‚¤ë¥¼ í‘œì¤€í™”ëœ í‚¤ë¡œ ë³€í™˜"""
    out = {}
    for k, v in d.items():
        nk = ALIASES.get(k, k)
        out[nk] = v
    return out

EMPTY_TOKENS = {"", "-", "â€”", "â€“", "ã…¡", "ì—†ìŒ", "ë¬´", "n/a", "na", "null", "none", "ë¯¸ì…ë ¥", "ë¯¸ê¸°ì¬", "í•´ë‹¹ì—†ìŒ"}

def is_empty(v) -> bool:
    """ìˆ«ì 0ì€ ê°’ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼(ì˜ˆ: ì²¨ë¶€ 0ê±´), ê·¸ ì™¸ í† í°/ê³µë°±ì€ ë¹ˆì¹¸"""
    if v is None:
        return True
    if isinstance(v, (int, float)):
        # 0.0, 0, -0.0 ë“±ì€ ê°’ì´ ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼. 
        # ê¸ˆì•¡ 0ì› ê²°ì¬ê°€ í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¡œì»¬ ê·œì¹™ì—ì„œ ë”°ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨.
        return False 
    s = str(v).strip().lower()
    if s in EMPTY_TOKENS:
        return True
    # êµ¬ë‘ì /ê³µë°±ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì¡Œë‹¤ë©´ ë¹ˆ ì¹¸
    if all((ch in string.punctuation) or ch.isspace() for ch in s):
        return True
    return False

# ==============================
# PDF â†’ Chroma ì €ì¥ / ê²€ìƒ‰
# ==============================
def save_pdf_to_chroma(chunks: List[str], embeddings: List[List[float]], source: str):
    """PDF í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ChromaDBì— ì €ì¥"""
    if not chroma_client:
        st.error("ChromaDBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    if not chunks or not embeddings:
        st.error(f"{source} PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    col = chroma_client.get_or_create_collection(GUIDE_COLLECTION_NAME)
    
    # ì´ì „ ë°ì´í„° ì‚­ì œ í›„ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
    col.delete(where={"source": source}) 
    
    base = 10_000 if source == "caution" else 0
    ids = [f"{source}_{base + i}" for i in range(len(chunks))]
    metas = [{"source": source, "chunk": i} for i in range(len(chunks))]
    
    col.add(ids=ids, documents=chunks, metadatas=metas, embeddings=embeddings)
    st.success(f"{source} {len(chunks)}ê°œ ì €ì¥ ì™„ë£Œ âœ…")

def search_guideline(query: str, api_key: str, k: int = 4) -> List[str]:
    """ChromaDBì—ì„œ ê°€ì´ë“œë¼ì¸ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    if not chroma_client:
        return ["ChromaDBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ê°€ì´ë“œë¼ì¸ì„ ê²€ìƒ‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]

    try:
        col = chroma_client.get_or_create_collection(GUIDE_COLLECTION_NAME)
        # ê²€ìƒ‰ ì‹œì—ë„ ë™ì¼í•˜ê²Œ text-embedding-3-small ì‚¬ìš©
        embedder = OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key) 
        q_emb = embedder.embed_query(query)
        result = col.query(query_embeddings=[q_emb], n_results=k)
        
        if not result or not result.get("documents"):
            return []
        
        texts = []
        for doc_list in result["documents"]:
            texts.extend(doc_list)
        return texts
    except Exception as e:
        # st.error(f"ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}") # ë””ë²„ê¹…ìš©
        return [f"ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"]

# ==============================
# Vision: ê²°ì¬ ë¬¸ì„œ ë¶„ì„ (ê°•ì œ JSON)
# ==============================
def gpt_extract_table(api_key: str, img: Image.Image, model: str = "gpt-4o") -> Dict[str, Any]:
    """GPT Visionì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í•µì‹¬ ê²°ì¬ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œ"""
    client = OpenAI(api_key=api_key)
    img_b64 = pil_to_b64(img)

    system_msg = (
        "ë„ˆëŠ” íšŒì‚¬ ê²°ì¬/ê²½ë¹„ ë¬¸ì„œë¥¼ í‘œ í˜•íƒœë¡œ ë¶„ì„í•˜ëŠ” AIë‹¤. "
        "ê²°ì¬ì„ (ê²°ì¬, ìŠ¹ì¸, ì°¸ì¡° ë“±)ì€ ë¬´ì‹œí•˜ê³ , ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•´ë¼."
    )
    user_msg = (
        "ì•„ë˜ ê²°ì¬ì„œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ë§Œë“¤ì–´ë¼.\n"
        "í•„ìˆ˜ í‚¤: ['ì œëª©','ì¦ë¹™ìœ í˜•','ì§€ê¸‰ìš”ì²­ì¼','ê¸ˆì•¡','attachment_count']\n" # â¬… ê¸ˆì•¡ ì¶”ê°€
        "ê¸ˆì•¡ì€ ì›í™” ê¸°í˜¸ ì—†ì´ ìˆ«ìí˜•(int/float)ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤. ì°¾ì§€ ëª»í•œ í‚¤ëŠ” ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” 0ìœ¼ë¡œ ë‘”ë‹¤. attachment_countëŠ” ì²¨ë¶€ ê±´ìˆ˜ë¥¼ ìˆ«ìí˜•ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤."
    )

    resp = client.chat.completions.create(
        model=model,
        temperature=0,
        response_format={"type": "json_object"},  # â¬… ê°•ì œ JSON
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": [
                {"type": "text", "text": user_msg},
                {"type": "image_url", "image_url": {"url": img_b64}},
            ]},
        ],
    )

    data = json.loads(resp.choices[0].message.content)

    # í‚¤ í†µì¼/ì •ê·œí™”
    data = normalize_keys(data)

    # attachment_count ìˆ«ìí™” ë° ê¸°ë³¸ê°’ ë³´ì •
    for key in ["attachment_count", "ê¸ˆì•¡"]:
        if isinstance(data.get(key), str):
            try:
                # ìˆ«ìë§Œ ì¶”ì¶œ (ex: '3ê±´' -> 3, '1,000,000ì›' -> 1000000)
                num_str = re.sub(r'[^0-9\.\-]', '', data[key].replace(',', ''))
                if num_str:
                    if '.' in num_str:
                        data[key] = float(num_str)
                    else:
                        data[key] = int(num_str)
                else:
                    data[key] = 0 if key == "attachment_count" else ""
            except Exception:
                data[key] = 0 if key == "attachment_count" else ""

    # ê¸°ë³¸ê°’ ë³´ì •
    data.setdefault("ì œëª©", "")
    data.setdefault("ì¦ë¹™ìœ í˜•", "")
    data.setdefault("ì§€ê¸‰ìš”ì²­ì¼", "")
    data.setdefault("ê¸ˆì•¡", "") # ê¸ˆì•¡ì€ ë¬¸ìì—´ë¡œ ë‚¨ê²¨ì„œ is_empty ì²´í¬ì— í¬í•¨ì‹œí‚¬ ìˆ˜ë„ ìˆìŒ
    data.setdefault("attachment_count", 0)

    return data

# ==============================
# ë¡œì»¬ ê·œì¹™ ê²€ì‚¬ (FAIL ëª…í™•í™” í•µì‹¬)
# ==============================
def check_local_rules(test_json: Dict[str, Any]) -> List[str]:
    """LLM í˜¸ì¶œ ì „, ì½”ë“œ ê¸°ë°˜ì˜ ëª…ì‹œì ì¸ FAIL ê·œì¹™ì„ ê²€ì‚¬"""
    local_fail_reasons = []

    # 1. í•„ìˆ˜ í‚¤ ë¹ˆì¹¸ ê²€ì‚¬ (FAIL í™•ì • 1ìˆœìœ„)
    # íšŒì‚¬ ê·œì •ì— ë”°ë¼ ê¸ˆì•¡, ê±°ë˜ì²˜ ë“± ì¶”ê°€
    required_keys = ["ì œëª©", "ì¦ë¹™ìœ í˜•", "ì§€ê¸‰ìš”ì²­ì¼", "ê¸ˆì•¡"] 
    for k in required_keys:
        if is_empty(test_json.get(k)):
            local_fail_reasons.append(f"í•„ìˆ˜ í•­ëª© '{k}'ì´(ê°€) ë¹„ì–´ ìˆìŒ")

    # 2. ë‚ ì§œ í˜•ì‹ ìœ íš¨ì„± ê²€ì‚¬ (YYYY-MM-DD í˜•ì‹ ì˜ˆìƒ)
    payment_date_str = str(test_json.get("ì§€ê¸‰ìš”ì²­ì¼", "")).strip()
    if payment_date_str:
        # YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD ë“± ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹ ê²€ì‚¬
        if not re.match(r"^\d{4}[\-\/\.]\d{1,2}[\-\/\.]\d{1,2}$", payment_date_str):
             local_fail_reasons.append(f"'ì§€ê¸‰ìš”ì²­ì¼'ì˜ í˜•ì‹ ì˜¤ë¥˜: '{payment_date_str}' (YYYY-MM-DD ë“± ê·œì •ëœ í˜•ì‹ ë¶ˆì¼ì¹˜)")
        else:
            # ìœ íš¨í•œ ë‚ ì§œì¸ì§€ ì¶”ê°€ ê²€ì‚¬ (ì˜ˆ: 2023-13-40 ê°™ì€ ì˜¤ë¥˜ ë°©ì§€)
            try:
                # ì²« ë²ˆì§¸ ë§¤ì¹­ëœ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„ (ë¶„ë¦¬ ê¸°í˜¸ í†µì¼)
                date_parts = re.split(r'[\-\/\.]', payment_date_str)
                if len(date_parts) == 3:
                     datetime(int(date_parts[0]), int(date_parts[1]), int(date_parts[2]))
            except ValueError:
                local_fail_reasons.append(f"'ì§€ê¸‰ìš”ì²­ì¼'ì˜ ë‚ ì§œ ê°’ ì˜¤ë¥˜: '{payment_date_str}' (ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ ê°’)")


    # 3. ê¸ˆì•¡ ìœ íš¨ì„± ê²€ì‚¬ (ìŒìˆ˜/ë¹„ì •ìƒ ê°’)
    amount = test_json.get("ê¸ˆì•¡")
    if isinstance(amount, (int, float)):
        if amount < 0:
            local_fail_reasons.append(f"'ê¸ˆì•¡'ì´ ìŒìˆ˜ì„: {amount} (ë¹„ì •ìƒ ê°’)")
    # ê¸ˆì•¡ì´ ìˆì–´ì•¼ í•˜ëŠ” ë¬¸ì„œì¸ë° ê¸ˆì•¡ì´ 0ì›ì´ë¼ë©´ ê²½ê³ /FAIL ì²˜ë¦¬ (íšŒì‚¬ ê·œì • ë”°ë¼ ì¡°ì •)
    elif amount == 0 and not is_empty(test_json.get("ì¦ë¹™ìœ í˜•")):
        local_fail_reasons.append(f"'ê¸ˆì•¡'ì´ 0ì›ì„: {amount} (ê¸ˆì•¡ ëˆ„ë½ ë˜ëŠ” 0ì› ê²°ì¬ ë¶ˆê°€)")

    # 4. í•„ìˆ˜ ì²¨ë¶€ ê·œì¹™ (íšŒì‚¬ ê·œì •ì— ë”°ë¼ ì¡°ì •)
    # ì˜ˆ: ì§€ê¸‰ìš”ì²­/ì¦ë¹™ìœ í˜•ì´ ìˆëŠ” ë¬¸ì„œ ì¤‘ íŠ¹ì • ìœ í˜•ì€ ì²¨ë¶€ 0ê±´ì´ë©´ FAIL
    must_attach_types = ["ì¹´ë“œì˜ìˆ˜ì¦", "ì„¸ê¸ˆê³„ì‚°ì„œ", "ì„¸ê¸ˆ ê³„ì‚°ì„œ", "ì†¡ê¸ˆí™•ì¸ì¦", "ê³„ì•½ì„œ"] # â¬… ì²¨ë¶€ í•„ìˆ˜ ì¦ë¹™ ìœ í˜•
    doc_type = test_json.get("ì¦ë¹™ìœ í˜•", "").strip()

    if doc_type in must_attach_types and test_json.get("attachment_count", 0) == 0:
        local_fail_reasons.append(f"ì¦ë¹™ìœ í˜• '{doc_type}'ì€(ëŠ”) ì²¨ë¶€ í•„ìˆ˜ í•­ëª©ì¸ë° attachment_count=0 (ì²¨ë¶€ ëˆ„ë½)")

    return local_fail_reasons

# ==============================
# PASS / FAIL ë°ì´í„° í•™ìŠµ ì €ì¥(ë©”íƒ€/ì¤‘ë³µ ë°©ì§€)
# ==============================
def save_pass_fail_data(new_data: List[Dict[str, Any]]):
    """í•™ìŠµ ë°ì´í„°ë¥¼ JSON íŒŒì¼ì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€)"""
    # /tmp ê²½ë¡œì— íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ íŒŒì¼ ìƒì„±
    if not os.path.exists(DATASET_PATH):
        with open(DATASET_PATH, "w", encoding="utf-8") as f:
            json.dump([], f)
    
    with open(DATASET_PATH, "r", encoding="utf-8") as f:
        try:
            existing = json.load(f)
        except json.JSONDecodeError:
             existing = []
             st.warning(f"ê¸°ì¡´ {DATASET_PATH} íŒŒì¼ì´ ì†ìƒë˜ì–´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")

    # ì¤‘ë³µ ë°©ì§€ìš© í•´ì‹œ ì§‘í•©
    seen = {d.get("_hash") for d in existing if d.get("_hash")}

    saved = 0
    for d in new_data:
        d = normalize_keys(d)
        meta_str = json.dumps(d, ensure_ascii=False, sort_keys=True)
        _hash = hashlib.md5(meta_str.encode()).hexdigest()
        if _hash in seen:
            continue
        d["_hash"] = _hash
        d["_saved_at"] = datetime.now().isoformat(timespec="seconds")
        d["_doc_type"] = (d.get("ì œëª©") or "").split()[0]
        existing.append(d)
        seen.add(_hash)
        saved += 1

    with open(DATASET_PATH, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)
    st.success(f"PASS/FAIL ë°ì´í„° {saved}ê±´ ì €ì¥ ì™„ë£Œ (ì´ {len(existing)}ê±´) âœ…")

# ==============================
# í†µí•© ë¹„êµ (ê°€ì´ë“œë¼ì¸ + íŒ¨í„´ í•™ìŠµ)
# ==============================
def integrated_compare(api_key: str, test_json: Dict[str, Any], local_fail_reasons: List[str], model: str = "gpt-4o") -> str:
    """RAGì™€ PASS/FAIL í•™ìŠµ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… íŒì •"""
    llm = ChatOpenAI(model=model, temperature=0.1, api_key=api_key, max_tokens=2500) # temperatureë¥¼ 0.1ë¡œ ì•½ê°„ ë†’ì—¬ ì¶”ë¡  ìœ ì—°ì„± í™•ë³´

    # RAG ê²€ìƒ‰ (ê°€ì´ë“œë¼ì¸)
    rag_texts: List[str] = []
    # ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¥¼ í†µí•´ ê°€ì´ë“œë¼ì¸ ì²­í¬ë¥¼ ê²€ìƒ‰
    for q in [
        f"'{test_json.get('ì œëª©', 'ë¬¸ì„œ')}'ì— ëŒ€í•œ ê·œì •",
        f"'{test_json.get('ì¦ë¹™ìœ í˜•', 'ì¦ë¹™')}' ì²˜ë¦¬ ê·œì¹™",
        "ì§€ê¸‰ìš”ì²­ì¼ í˜•ì‹ ë° ê·œì¹™",
        "ì²¨ë¶€íŒŒì¼ ê·œì¹™",
        "ê²½ë¹„ì²­êµ¬ ì£¼ì˜ì‚¬í•­"
    ]:
        rag_texts.extend(search_guideline(q, api_key, k=2))
    
    # ì¤‘ë³µ ì œê±°
    rag_texts = list(set(rag_texts))

    # PASS/FAIL í•™ìŠµ ë°ì´í„° ë¡œë“œ
    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ íŒŒì¼ ìƒì„± (Streamlit Cloudì—ì„œ ì²« ì‹¤í–‰ ì‹œ íŒŒì¼ì´ ì—†ê¸° ë•Œë¬¸)
    if not os.path.exists(DATASET_PATH):
        with open(DATASET_PATH, "w", encoding="utf-8") as f:
            json.dump([], f)

    if os.path.exists(DATASET_PATH):
        with open(DATASET_PATH, "r", encoding="utf-8") as f:
            try:
                dataset = json.load(f)
            except json.JSONDecodeError:
                dataset = []

        pass_data = [d for d in dataset if d.get("ìƒíƒœ") == "PASS"]
        fail_data = [d for d in dataset if d.get("ìƒíƒœ") == "FAIL"]
    else:
        pass_data, fail_data = [], []


    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (FAIL ìš°ì„ ìˆœìœ„ ëª…í™•í™”)
    prompt = f"""
ë„ˆëŠ” íšŒì‚¬ ê²°ì¬ ë¬¸ì„œë¥¼ ì‚¬ì „ ê²€í† í•˜ëŠ” AIë‹¤.

[ê°€ì¥ ì¤‘ìš”í•œ ì›ì¹™: ë¡œì»¬ ê·œì¹™ FAILì€ ìµœì¢… FAIL]
1. ì•„ë˜ 'ë¡œì»¬ ê·œì¹™ì—ì„œ ê°ì§€ëœ FAIL ì‚¬ìœ 'ì— í•­ëª©ì´ **ë‹¨ í•˜ë‚˜ë¼ë„** ìˆìœ¼ë©´, ë‹¤ë¥¸ ì–´ë–¤ ì •ë³´(ê°€ì´ë“œë¼ì¸, PASS/FAIL ì˜ˆì‹œ ë“±)ë³´ë‹¤ ìš°ì„ í•˜ì—¬ **ìµœì¢… íŒì •ì€ ë°˜ë“œì‹œ 'FAIL'**ë¡œ ë‚´ë ¤ì•¼ í•œë‹¤.
2. ë¡œì»¬ FAIL ì‚¬ìœ ê°€ ì—†ë‹¤ë©´, [íšŒì‚¬ ê°€ì´ë“œë¼ì¸/ìœ ì˜ì‚¬í•­]ê³¼ [PASS/FAIL ì˜ˆì‹œ]ë¥¼ ì°¸ê³ í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ íŒë‹¨í•œë‹¤.

[ë¡œì»¬ ê·œì¹™ì—ì„œ ê°ì§€ëœ FAIL ì‚¬ìœ ] # â¬… ëª…ì‹œì  ì½”ë“œ ê²€ì¦ FAIL ì‚¬ìœ 
{json.dumps(local_fail_reasons, ensure_ascii=False, indent=2)}

[íšŒì‚¬ ê°€ì´ë“œë¼ì¸ ë° ìœ ì˜ì‚¬í•­ ì¼ë¶€] # â¬… RAGë¡œ ê²€ìƒ‰ëœ ì°¸ê³  ìë£Œ
{json.dumps(rag_texts[:10], ensure_ascii=False, indent=2)}

[PASS ë¬¸ì„œ ì˜ˆì‹œ(ì¼ë¶€)] # â¬… í•™ìŠµ íŒ¨í„´
{json.dumps(pass_data[:5], ensure_ascii=False, indent=2)}

[FAIL ë¬¸ì„œ ì˜ˆì‹œ(ì¼ë¶€)] # â¬… í•™ìŠµ íŒ¨í„´
{json.dumps(fail_data[:5], ensure_ascii=False, indent=2)}

[ê²€í† í•  ê²°ì¬ ë¬¸ì„œ(JSON)]
{json.dumps(test_json, ensure_ascii=False, indent=2)}

ìš”êµ¬ì‚¬í•­:
1) ë°œê²¬ëœ ë¬¸ì œë¥¼ ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë‚˜ì—´í•œë‹¤. ë¡œì»¬ FAIL ì‚¬ìœ ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ë§¨ ìœ„ì— ëª…ì‹œí•œë‹¤.
- í•­ëª©ëª…: ...
- ë¬¸ì œì : ...
- ìˆ˜ì • ì˜ˆì‹œ: ...

2) ë§¨ ë§ˆì§€ë§‰ ì¤„ì— 'ìµœì¢… íŒì •: PASS' ë˜ëŠ” 'ìµœì¢… íŒì •: FAIL (ì´ìœ  ...)' í˜•íƒœë¡œ ì¶œë ¥í•œë‹¤. ë¡œì»¬ FAIL ì‚¬ìœ ê°€ ìˆì„ ê²½ìš° í•´ë‹¹ ì‚¬ìœ ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ìµœì¢… íŒì •ì„ ë‚´ë¦°ë‹¤.
3) ë¶ˆí•„ìš”í•œ ì„œë¡ ì€ ì“°ì§€ ë§ê³  í˜•ì‹ë§Œ ì§€ì¼œë¼.
"""
    try:
        res = llm.invoke([HumanMessage(content=prompt)])
        return res.content if hasattr(res, "content") else str(res)
    except Exception as e:
        return f"âŒ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# ==============================
# UI
# ==============================
with st.sidebar:
    st.subheader("ğŸ”‘ OpenAI ì„¤ì •")
    # API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ìœ ì§€
    api_key = st.text_input("OpenAI API Key", type="password", value=os.environ.get("OPENAI_API_KEY", ""))
    model = st.selectbox("ëª¨ë¸ ì„ íƒ", ["gpt-4o", "gpt-4o-mini"], index=0)
    st.caption("ê°€ì´ë“œë¼ì¸ PDF ì„ë² ë”© ìƒì„± â†’ PASS/FAIL í•™ìŠµ(ì„ íƒ) â†’ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ ìˆœì„œë¡œ ì§„í–‰í•˜ì„¸ìš”.")

col1, col2 = st.columns([1.1, 0.9])

# ---------------- ì™¼ìª½: ë°ì´í„° ì—…ë¡œë“œ ----------------
with col1:
    st.header("â‘  ê°€ì´ë“œë¼ì¸ / ìœ ì˜ì‚¬í•­ ì—…ë¡œë“œ")
    pdf_file = st.file_uploader("ê°€ì´ë“œë¼ì¸ PDF", type=["pdf"], key="guide_pdf")
    caution_pdf = st.file_uploader("ìœ ì˜ì‚¬í•­ PDF", type=["pdf"], key="caution_pdf")

    if st.button("PDF ì„ë² ë”© ìƒì„±/ì—…ë°ì´íŠ¸"):
        if not api_key:
            st.error("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            if pdf_file:
                st.info(f"ê°€ì´ë“œë¼ì¸ PDF ({pdf_file.name}) ì²˜ë¦¬ ì¤‘...")
                text = pdf_to_text(pdf_file.read())
                if text.strip():
                    chunks = split_text(text)
                    embs = embed_texts(chunks, api_key)
                    save_pdf_to_chroma(chunks, embs, "guideline")
            
            if caution_pdf:
                st.info(f"ìœ ì˜ì‚¬í•­ PDF ({caution_pdf.name}) ì²˜ë¦¬ ì¤‘...")
                text = pdf_to_text(caution_pdf.read())
                if text.strip():
                    chunks = split_text(text)
                    embs = embed_texts(chunks, api_key)
                    save_pdf_to_chroma(chunks, embs, "caution")

    st.header("â‘¡ PASS / FAIL í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ (ì„ íƒ)")
    pass_imgs = st.file_uploader("âœ… PASS ë¬¸ì„œ (ì—¬ëŸ¬ì¥ ê°€ëŠ¥)", type=["jpg", "png", "jpeg"], accept_multiple_files=True)
    fail_imgs = st.file_uploader("âŒ FAIL ë¬¸ì„œ (ì—¬ëŸ¬ì¥ ê°€ëŠ¥)", type=["jpg", "png", "jpeg"], accept_multiple_files=True)

    if st.button("PASS/FAIL ë°ì´í„° í•™ìŠµ"):
        if not api_key:
            st.error("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            learned = []
            
            # PASS ë¬¸ì„œ ì²˜ë¦¬
            for img_file in pass_imgs or []:
                st.info(f"PASS ë¬¸ì„œ ì¸ì‹ ì¤‘: {img_file.name}")
                img = Image.open(img_file).convert("RGB")
                data = gpt_extract_table(api_key, img, model)
                data["ìƒíƒœ"] = "PASS"
                learned.append(data)
                
            # FAIL ë¬¸ì„œ ì²˜ë¦¬
            for img_file in fail_imgs or []:
                st.info(f"FAIL ë¬¸ì„œ ì¸ì‹ ì¤‘: {img_file.name}")
                img = Image.open(img_file).convert("RGB")
                data = gpt_extract_table(api_key, img, model)
                data["ìƒíƒœ"] = "FAIL"
                learned.append(data)
                
            if learned:
                save_pass_fail_data(learned)
            else:
                st.info("ì—…ë¡œë“œëœ PASS/FAIL ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.header("â‘¢ í…ŒìŠ¤íŠ¸ ê²°ì¬ ë¬¸ì„œ ì—…ë¡œë“œ")
    test_img = st.file_uploader("í…ŒìŠ¤íŠ¸ ë¬¸ì„œ", type=["jpg", "png", "jpeg"], key="test_img")
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ ì¦‰ì‹œ ë¶„ì„ ì‹¤í–‰ (UX ê°œì„ )
    if test_img and api_key:
        try:
            img = Image.open(test_img).convert("RGB")
            st.image(img, caption="í…ŒìŠ¤íŠ¸ ë¬¸ì„œ", use_container_width=True)
            
            with st.spinner("ë¬¸ì„œ ì¸ì‹ ì¤‘ (GPT-Vision)..."):
                test_json = gpt_extract_table(api_key, img, model)
            
            # í‚¤ í†µì¼
            test_json = normalize_keys(test_json)
            st.session_state["test_json"] = test_json
            st.session_state["local_fail_reasons"] = check_local_rules(test_json) # â¬… ë¡œì»¬ ê·œì¹™ ì¦‰ì‹œ ê²€ì‚¬
            
            st.success("ë¬¸ì„œ ì¸ì‹ ì™„ë£Œ ë° ë¡œì»¬ ê·œì¹™ ê²€ì‚¬ ì™„ë£Œ âœ…")
            st.markdown("**ë¬¸ì„œ ì¸ì‹ ê²°ê³¼ (JSON)**")
            st.code(json.dumps(test_json, ensure_ascii=False, indent=2), language="json")

        except Exception as e:
             st.error(f"ë¬¸ì„œ ì¸ì‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
             st.session_state["test_json"] = None
             st.session_state["local_fail_reasons"] = []
    
    elif test_img and not api_key:
        st.error("í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ë ¤ë©´ OpenAI API Keyë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        st.session_state["test_json"] = None
        st.session_state["local_fail_reasons"] = []

# ---------------- ì˜¤ë¥¸ìª½: ê²°ê³¼ ----------------
with col2:
    st.header("â‘£ AI í†µí•© ê²€í†  ê²°ê³¼")
    
    test_json_data = st.session_state.get("test_json")
    local_fails = st.session_state.get("local_fail_reasons", [])

    if local_fails and test_json_data:
        st.error("ğŸš¨ **ë¡œì»¬ ê·œì¹™ ìœ„ë°˜ ê°ì§€:**")
        st.warning("ì•„ë˜ì˜ ëª…ì‹œì  FAIL ì‚¬ìœ ê°€ ìˆì–´ ìµœì¢… íŒì •ì€ **FAIL**ì´ ë  ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.")
        st.markdown("\n".join([f"- **{reason}**" for reason in local_fails]))
        st.markdown("---") # ì‹œê°ì  êµ¬ë¶„
    elif test_json_data:
        st.info("ë¡œì»¬ ê·œì¹™ ê²€ì‚¬: ëª…ì‹œì ì¸ FAIL ì‚¬ìœ ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì œ AI í†µí•© ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        st.markdown("---")


    if st.button("AI ìë™ ê²€í†  ì‹¤í–‰", disabled=not test_json_data):
        if not api_key:
            st.error("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif not test_json_data:
            st.error("í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•˜ê³  ì¸ì‹ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            with st.spinner("AIê°€ ê·œì • + í•™ìŠµ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„ ì¤‘... (RAG + Pattern Learning)"):
                result = integrated_compare(api_key, test_json_data, local_fails, model)
            
            st.success("ê²€í†  ì™„ë£Œ âœ…")
            st.markdown("#### **ê²€í†  ê²°ê³¼ (AI ë¶„ì„)**")
            
            # ìµœì¢… íŒì •ì— ë”°ë¼ ì‹œê°ì  ê°•ì¡°
            if "ìµœì¢… íŒì •: FAIL" in result:
                st.error(result)
            else:
                st.success(result)
            
            # st.write(result)


# ==============================
# ë°ì´í„°ì…‹ í™•ì¸ìš© (ì„ íƒì )
# ==============================
if st.checkbox("PASS/FAIL í•™ìŠµ ë°ì´í„° ëª©ë¡ ë³´ê¸°"):
    if os.path.exists(DATASET_PATH):
        with open(DATASET_PATH, "r", encoding="utf-8") as f:
            try:
                dataset = json.load(f)
                st.dataframe(dataset)
            except json.JSONDecodeError:
                st.error("PASS/FAIL ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜")
    else:
        st.info("ì €ì¥ëœ PASS/FAIL í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
