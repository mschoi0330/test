# app.py â€” RAG + PASS/FAIL ì˜ˆì‹œ ê¸°ë°˜ + ìœ ì‚¬ë„ top-k ì„ ë³„
# í¬í•¨: ê°•ì œ JSON, í‚¤ ì •ê·œí™”/ì¼ê´€í™”, ë¹ˆì¹¸ í† í°, PASS/FAIL ë©”íƒ€Â·ì¤‘ë³µ ë°©ì§€,
# ì—…ë¡œë“œ-ì¦‰ì‹œ ë¶„ì„, ìŠ¤ìº”PDF ê²½ê³ , ë¡œì»¬ ë£° ìš°ì„ , ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì‹œ ì„ ë³„

import streamlit as st
import os, io, json, base64, glob, re, hashlib, unicodedata, string
from datetime import datetime
from typing import List, Dict, Any, Tuple
from PIL import Image
from openai import OpenAI
from pypdf import PdfReader
import chromadb
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import numpy as np

# ==============================
# ì„¤ì •
# ==============================
APP_TITLE = "ğŸ“„ AI ê²°ì¬ ì‚¬ì „ê²€í† "
DB_DIR = "./chroma_db"
DATASET_PATH = "./pass_fail_dataset.json"
GUIDE_COLLECTION_NAME = "company_guideline"
TOPK_SIMILAR = 3  # PASS/FAIL ê°ê° top-k

st.set_page_config(page_title=APP_TITLE, layout="wide")
st.title(APP_TITLE)

# ==============================
# ì´ˆê¸°í™”
# ==============================
chroma_client = chromadb.PersistentClient(path=DB_DIR)

# ==============================
# ê³µí†µ ìœ í‹¸
# ==============================
def pdf_to_text(file: bytes) -> str:
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFëŠ” ë°”ë¡œ ì¶”ì¶œ, ìŠ¤ìº” PDFë©´ ë¹ˆ ë¬¸ìì—´ ê°€ëŠ¥"""
    try:
        reader = PdfReader(io.BytesIO(file))
        texts = [page.extract_text() or "" for page in reader.pages]
        text = "\n".join(texts)
        if not text.strip():
            st.warning("âš ï¸ PDFì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤ìº” PDFì¼ ìˆ˜ ìˆì–´ìš”.")
        return text
    except Exception as e:
        st.error(f"PDF ì½ê¸° ì˜¤ë¥˜: {e}")
        return ""

def split_text(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    text = text.replace("\r", "\n")
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(L, start + chunk_size)
        chunk = text[start:end]
        if chunk.strip():
            chunks.append(chunk)
        start = end - overlap
        if start < 0: start = 0
        if start >= L: break
    return chunks

def embed_texts(texts: List[str], api_key: str) -> List[List[float]]:
    if not texts: return []
    emb = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)
    return emb.embed_documents(texts)

def pil_to_b64(img: Image.Image) -> str:
    buf = io.BytesIO(); img.save(buf, format="PNG")
    return "data:image/png;base64," + base64.b64encode(buf.getvalue()).decode("utf-8")

# ==============================
# í‚¤ ì •ê·œí™” / ë¹ˆì¹¸ íŒì •
# ==============================
ALIASES = {
    # ì²¨ë¶€/ì¦ë¹™ ê´€ë ¨ â†’ attachment_countë¡œ í†µì¼
    "ì²¨ë¶€íŒŒì¼ìˆ˜": "attachment_count",
    "ì²¨ë¶€": "attachment_count",
    "ì¦ë¹™ê°œìˆ˜": "attachment_count",
    "ì²¨ë¶€(ê±´ìˆ˜)": "attachment_count",
}

def normalize_keys(d: Dict[str, Any]) -> Dict[str, Any]:
    out = {}
    for k, v in d.items():
        out[ALIASES.get(k, k)] = v
    return out

EMPTY_TOKENS = {"", "-", "â€”", "â€“", "ã…¡", "ì—†ìŒ", "ë¬´", "n/a", "na", "null", "none", "ë¯¸ì…ë ¥", "ë¯¸ê¸°ì¬", "í•´ë‹¹ì—†ìŒ"}

def is_empty(v) -> bool:
    """ìˆ«ì 0ì€ ê°’ìœ¼ë¡œ ê°„ì£¼(ì˜ˆ: ì²¨ë¶€ 0ê±´), ê·¸ ì™¸ í† í°/ê³µë°±/êµ¬ë‘ì ë§Œì´ë©´ ë¹ˆì¹¸"""
    if v is None:
        return True
    if isinstance(v, (int, float)):
        return False
    s = str(v).strip().lower()
    if s in EMPTY_TOKENS:
        return True
    if all((ch in string.punctuation) or ch.isspace() for ch in s):
        return True
    return False

def dict_to_sorted_text(d: Dict[str, Any]) -> str:
    """ì„ë² ë”©ìš© ë¬¸ìì—´ ë³€í™˜: í‚¤ ì •ë ¬ í›„ 'k: v' ì¤„ë¡œ í•©ì¹¨"""
    try:
        items = sorted(d.items(), key=lambda x: str(x[0]))
        return "\n".join(f"{k}: {v}" for k, v in items if not str(k).startswith("_"))
    except Exception:
        return json.dumps(d, ensure_ascii=False, sort_keys=True)

def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
    na = np.linalg.norm(a); nb = np.linalg.norm(b)
    if na == 0 or nb == 0: return 0.0
    return float(np.dot(a, b) / (na * nb))

# ==============================
# PDF â†’ Chroma ì €ì¥ / ê²€ìƒ‰
# ==============================
def save_pdf_to_chroma(chunks: List[str], embeddings: List[List[float]], source: str):
    if not chunks or not embeddings:
        st.error(f"{source} PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    col = chroma_client.get_or_create_collection(GUIDE_COLLECTION_NAME)
    col.delete(where={"source": source})
    base = 10_000 if source == "caution" else 0
    ids = [f"{source}_{base + i}" for i in range(len(chunks))]
    metas = [{"source": source, "chunk": i} for i in range(len(chunks))]
    col.add(ids=ids, documents=chunks, metadatas=metas, embeddings=embeddings)
    st.success(f"{source} {len(chunks)}ê°œ ì €ì¥ ì™„ë£Œ âœ…")

def search_guideline(query: str, api_key: str, k: int = 4) -> List[str]:
    col = chroma_client.get_or_create_collection(GUIDE_COLLECTION_NAME)
    emb = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)
    q_emb = emb.embed_query(query)
    result = col.query(query_embeddings=[q_emb], n_results=k)
    if not result or not result.get("documents"): return []
    return [result["documents"][0][i] for i in range(len(result["documents"][0]))]

# ==============================
# Vision: ê²°ì¬ ë¬¸ì„œ ë¶„ì„ (ê°•ì œ JSON)
# ==============================
def gpt_extract_table(api_key: str, img: Image.Image, model: str = "gpt-4o") -> Dict[str, Any]:
    client = OpenAI(api_key=api_key)
    img_b64 = pil_to_b64(img)

    system_msg = (
        "ë„ˆëŠ” íšŒì‚¬ ê²°ì¬/ê²½ë¹„ ë¬¸ì„œë¥¼ í‘œ í˜•íƒœë¡œ ë¶„ì„í•˜ëŠ” AIë‹¤. "
        "ê²°ì¬ì„ (ê²°ì¬, ìŠ¹ì¸, ì°¸ì¡° ë“±)ì€ ë¬´ì‹œí•˜ê³ , ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•´ë¼."
    )
    user_msg = (
        "ì•„ë˜ ê²°ì¬ì„œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ë§Œë“¤ì–´ë¼.\n"
        "í•„ìˆ˜ í‚¤: ['ì œëª©','ì¦ë¹™ìœ í˜•','ì§€ê¸‰ìš”ì²­ì¼','attachment_count']\n"
        "ê°€ëŠ¥í•˜ë©´ ìˆ«ìëŠ” ìˆ«ìí˜•ìœ¼ë¡œ. ì°¾ì§€ ëª»í•œ í‚¤ëŠ” ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” 0ìœ¼ë¡œ ë‘”ë‹¤."
    )

    resp = client.chat.completions.create(
        model=model,
        temperature=0,
        response_format={"type": "json_object"},  # ê°•ì œ JSON
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": [
                {"type": "text", "text": user_msg},
                {"type": "image_url", "image_url": {"url": img_b64}},
            ]},
        ],
    )

    data = json.loads(resp.choices[0].message.content)
    data = normalize_keys(data)

    # attachment_count ìˆ«ìí™”
    if isinstance(data.get("attachment_count"), str):
        try:
            nums = re.findall(r"\d+", data["attachment_count"])
            data["attachment_count"] = int(nums[0]) if nums else 0
        except Exception:
            data["attachment_count"] = 0

    data.setdefault("ì œëª©", "")
    data.setdefault("ì¦ë¹™ìœ í˜•", "")
    data.setdefault("ì§€ê¸‰ìš”ì²­ì¼", "")
    data.setdefault("attachment_count", 0)
    return data

# ==============================
# PASS / FAIL ë°ì´í„° í•™ìŠµ ì €ì¥(ë©”íƒ€/ì¤‘ë³µ ë°©ì§€)
# ==============================
def save_pass_fail_data(new_data: List[Dict[str, Any]]):
    if os.path.exists(DATASET_PATH):
        with open(DATASET_PATH, "r", encoding="utf-8") as f:
            existing = json.load(f)
    else:
        existing = []

    seen = {d.get("_hash") for d in existing if d.get("_hash")}
    saved = 0
    for d in new_data:
        d = normalize_keys(d)
        meta_str = json.dumps(d, ensure_ascii=False, sort_keys=True)
        _hash = hashlib.md5(meta_str.encode()).hexdigest()
        if _hash in seen:  # ì¤‘ë³µ ë°©ì§€
            continue
        d["_hash"] = _hash
        d["_saved_at"] = datetime.now().isoformat(timespec="seconds")
        d["_doc_type"] = (d.get("ì œëª©") or "").split()[0]
        existing.append(d)
        seen.add(_hash)
        saved += 1

    with open(DATASET_PATH, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)
    st.success(f"PASS/FAIL ë°ì´í„° {saved}ê±´ ì €ì¥ ì™„ë£Œ (ì´ {len(existing)}ê±´) âœ…")

# ==============================
# ìœ ì‚¬ ìƒ˜í”Œ ì„ ë³„ (ì„ë² ë”© top-k)
# ==============================
def select_similar_examples(api_key: str, test_json: Dict[str, Any], dataset: List[Dict[str, Any]], k: int = TOPK_SIMILAR) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    if not dataset: return [], []
    emb = OpenAIEmbeddings(model="text-embedding-3-large", api_key=api_key)

    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ë² ë”©
    test_text = dict_to_sorted_text(test_json)
    try:
        test_vec = np.array(emb.embed_query(test_text), dtype=float)
    except Exception:
        return [], []

    # PASS/FAIL ë¶„ë¦¬ í›„ ê°ê° ìœ ì‚¬ë„ top-k
    pass_docs = [d for d in dataset if d.get("ìƒíƒœ") == "PASS"]
    fail_docs = [d for d in dataset if d.get("ìƒíƒœ") == "FAIL"]

    def topk_for(group):
        scored = []
        for d in group:
            vec = np.array(emb.embed_query(dict_to_sorted_text(d)), dtype=float)
            sim = cosine_sim(test_vec, vec)
            scored.append((sim, d))
        scored.sort(key=lambda x: x[0], reverse=True)
        return [d for _, d in scored[:k]]

    return topk_for(pass_docs), topk_for(fail_docs)

# ==============================
# í†µí•© ë¹„êµ (ê°€ì´ë“œë¼ì¸ + íŒ¨í„´ í•™ìŠµ + ìœ ì‚¬ë„ ì˜ˆì‹œ)
# ==============================
def integrated_compare(api_key: str, test_json: Dict[str, Any], model: str = "gpt-4o") -> str:
    llm = ChatOpenAI(model=model, temperature=0, api_key=api_key, max_tokens=2500)

    # RAG ìŠ¤ë‹ˆí« ìˆ˜ì§‘
    rag_texts: List[str] = []
    for q in ["ì§€ê¸‰ìš”ì²­ì¼ ì…ë ¥ ê·œì¹™", "ì¦ë¹™ìœ í˜• ì…ë ¥ ê·œì¹™", "ì²¨ë¶€íŒŒì¼ ê·œì¹™", "ê²½ë¹„ì²­êµ¬ ì£¼ì˜ì‚¬í•­"]:
        rag_texts.extend(search_guideline(q, api_key, k=2))

    # ë°ì´í„°ì…‹ ë¡œë“œ ë° ìœ ì‚¬ ì˜ˆì‹œ ì„ ë³„
    if os.path.exists(DATASET_PATH):
        with open(DATASET_PATH, "r", encoding="utf-8") as f:
            dataset = json.load(f)
    else:
        dataset = []

    # ë¡œì»¬ ë£°: í•„ìˆ˜ í•­ëª©/ì²¨ë¶€ ëˆ„ë½ ì „ë¶€ ê¸°ë¡
    local_fail_reasons = []
    required_keys = ["ì œëª©", "ì¦ë¹™ìœ í˜•", "ì§€ê¸‰ìš”ì²­ì¼", "attachment_count"]
    for k in required_keys:
        if is_empty(test_json.get(k)):
            local_fail_reasons.append(f"í•„ìˆ˜ í•­ëª© '{k}'ì´(ê°€) ë¹„ì–´ ìˆìŒ")
    if not is_empty(test_json.get("ì¦ë¹™ìœ í˜•")) and test_json.get("attachment_count", 0) == 0:
        local_fail_reasons.append("ì¦ë¹™ìœ í˜• ì¡´ì¬í•˜ì§€ë§Œ ì²¨ë¶€ 0ê±´ (ì²¨ë¶€ ëˆ„ë½ ê°€ëŠ¥)")

    # ìœ ì‚¬ PASS/FAIL top-k ì„ ë³„
    pass_topk, fail_topk = select_similar_examples(api_key, test_json, dataset, k=TOPK_SIMILAR)

    prompt = f"""
ë„ˆëŠ” íšŒì‚¬ ê²°ì¬ ë¬¸ì„œë¥¼ ì‚¬ì „ ê²€í† í•˜ëŠ” AIë‹¤.

ë°˜ë“œì‹œ ì•„ë˜ ì¡°ê±´ì„ ì§€ì¼œë¼:
- ë¬¸ì„œì˜ ëª¨ë“  í•­ëª©ì„ ëê¹Œì§€ ê²€í† í•˜ë¼. í•˜ë‚˜ì˜ FAIL ì‚¬ìœ ë¥¼ ë°œê²¬í•´ë„ ë©ˆì¶”ì§€ ë§ê³ , **ëª¨ë“  ë¬¸ì œë¥¼ ì „ë¶€ ë‚˜ì—´**í•´ì•¼ í•œë‹¤.
- ê° ë¬¸ì œëŠ” ê°œë³„ í•­ëª©ë³„ë¡œ êµ¬ì²´ì ì¸ ì´ìœ ì™€ ìˆ˜ì • ì˜ˆì‹œë¥¼ í¬í•¨í•˜ë¼.

íŒì • ìš°ì„ ìˆœìœ„:
A. ì•„ë˜ 'ë¡œì»¬ ê·œì¹™'ì—ì„œ FAIL ì‚¬ìœ ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ FAILë¡œ ê°„ì£¼í•˜ê³ , ëª¨ë“  í•­ëª©ì— ëŒ€í•œ ë¬¸ì œì ì„ ë‚˜ì—´í•œë‹¤.
B. ê·¸ ì™¸ í•­ëª©ë„ PASS/FAIL ì˜ˆì‹œ ë° ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ì ì¸ ëˆ„ë½Â·ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ì œì‹œí•œë‹¤.

[ë¡œì»¬ ê·œì¹™ì—ì„œ ê°ì§€ëœ FAIL ì‚¬ìœ ]
{json.dumps(local_fail_reasons, ensure_ascii=False, indent=2)}

[íšŒì‚¬ ê°€ì´ë“œë¼ì¸ ë° ìœ ì˜ì‚¬í•­ ì¼ë¶€]
{json.dumps(rag_texts[:10], ensure_ascii=False, indent=2)}

[ìœ ì‚¬ PASS ë¬¸ì„œ ì˜ˆì‹œ(ìµœëŒ€ {TOPK_SIMILAR}ê°œ)]
{json.dumps(pass_topk, ensure_ascii=False, indent=2)}

[ìœ ì‚¬ FAIL ë¬¸ì„œ ì˜ˆì‹œ(ìµœëŒ€ {TOPK_SIMILAR}ê°œ)]
{json.dumps(fail_topk, ensure_ascii=False, indent=2)}

[ê²€í† í•  ê²°ì¬ ë¬¸ì„œ(JSON)]
{json.dumps(test_json, ensure_ascii=False, indent=2)}

ìš”êµ¬ì‚¬í•­:
1) ëª¨ë“  í•­ëª©ì„ ëê¹Œì§€ ê²€í† í•˜ì—¬ ë¬¸ì œë¥¼ ë¹ ì§ì—†ì´ ë‚˜ì—´í•œë‹¤.
2) ì¶œë ¥ì€ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¨ë¼.
- í•­ëª©ëª…: ...
- ë¬¸ì œì : ...
- ìˆ˜ì • ì˜ˆì‹œ: ...
3) ë§¨ ë§ˆì§€ë§‰ ì¤„ì— 'ìµœì¢… íŒì •: PASS' ë˜ëŠ” 'ìµœì¢… íŒì •: FAIL (ì´ìœ  ...)' í˜•íƒœë¡œ ì‘ì„±í•œë‹¤.
"""
    res = llm.invoke(prompt)
    return res.content if hasattr(res, "content") else str(res)

# ==============================
# UI
# ==============================
with st.sidebar:
    st.subheader("ğŸ”‘ OpenAI ì„¤ì •")
    api_key = st.text_input("OpenAI API Key", type="password", value=os.environ.get("OPENAI_API_KEY", ""))
    model = st.selectbox("ëª¨ë¸ ì„ íƒ", ["gpt-4o", "gpt-4o-mini"], index=0)
    st.caption("ê°€ì´ë“œë¼ì¸ PDF ì„ë² ë”© â†’ PASS/FAIL í•™ìŠµ(ì„ íƒ) â†’ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ ìˆœì„œë¡œ ì§„í–‰í•˜ì„¸ìš”.")

col1, col2 = st.columns([1.1, 0.9])

# ---------------- ì™¼ìª½: ë°ì´í„° ì—…ë¡œë“œ ----------------
with col1:
    st.header("â‘  ê°€ì´ë“œë¼ì¸ / ìœ ì˜ì‚¬í•­ ì—…ë¡œë“œ")
    pdf_file = st.file_uploader("ê°€ì´ë“œë¼ì¸ PDF", type=["pdf"], key="guide_pdf")
    caution_pdf = st.file_uploader("ìœ ì˜ì‚¬í•­ PDF", type=["pdf"], key="caution_pdf")

    if st.button("PDF ì„ë² ë”© ìƒì„±/ì—…ë°ì´íŠ¸"):
        if not api_key:
            st.error("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            if pdf_file:
                text = pdf_to_text(pdf_file.read())
                if text.strip():
                    chunks = split_text(text)
                    embs = embed_texts(chunks, api_key)
                    save_pdf_to_chroma(chunks, embs, "guideline")
            if caution_pdf:
                text = pdf_to_text(caution_pdf.read())
                if text.strip():
                    chunks = split_text(text)
                    embs = embed_texts(chunks, api_key)
                    save_pdf_to_chroma(chunks, embs, "caution")

    st.header("â‘¡ PASS / FAIL í•™ìŠµ ë°ì´í„° ì—…ë¡œë“œ (ì„ íƒ)")
    pass_imgs = st.file_uploader("âœ… PASS ë¬¸ì„œ (ì—¬ëŸ¬ì¥ ê°€ëŠ¥)", type=["jpg", "png", "jpeg"], accept_multiple_files=True)
    fail_imgs = st.file_uploader("âŒ FAIL ë¬¸ì„œ (ì—¬ëŸ¬ì¥ ê°€ëŠ¥)", type=["jpg", "png", "jpeg"], accept_multiple_files=True)

    if st.button("PASS/FAIL ë°ì´í„° í•™ìŠµ"):
        if not api_key:
            st.error("API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            learned = []
            for img_file in pass_imgs or []:
                img = Image.open(img_file).convert("RGB")
                data = gpt_extract_table(api_key, img, model)
                data["ìƒíƒœ"] = "PASS"
                learned.append(data)
            for img_file in fail_imgs or []:
                img = Image.open(img_file).convert("RGB")
                data = gpt_extract_table(api_key, img, model)
                data["ìƒíƒœ"] = "FAIL"
                learned.append(data)
            if learned:
                save_pass_fail_data(learned)
            else:
                st.info("ì—…ë¡œë“œëœ PASS/FAIL ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.header("â‘  ê²°ì¬ ë¬¸ì„œ ì—…ë¡œë“œ")
    test_img = st.file_uploader("ê²€í† í•  ë¬¸ì„œ (jpg/png)", type=["jpg", "png", "jpeg"])

    if test_img and api_key:
    # ì´ë¯¸ì§€ ì—´ê¸°
        img = Image.open(test_img).convert("RGB")
        st.image(img, caption="ì—…ë¡œë“œ ë¬¸ì„œ", use_container_width=True)

    # Vision í˜¸ì¶œë¡œ JSON ì¶”ì¶œ
        with st.spinner("ë¬¸ì„œ ì¸ì‹ ì¤‘..."):
            doc_json = gpt_extract_table(api_key, img, model="gpt-4o")  # â† ì—¬ê¸°ì„œ doc_json ìƒì„±

        st.success("ë¬¸ì„œ ì¸ì‹ ì™„ë£Œ âœ…")

    # UIì— JSONì€ ë³´ì—¬ì£¼ì§€ ì•Šê³  ì„¸ì…˜ì—ë§Œ ì €ì¥
        st.session_state["doc_json"] = doc_json

# ---------------- ì˜¤ë¥¸ìª½: ê²°ê³¼ ----------------
with col2:
    st.header("â‘£ AI í†µí•© ê²€í†  ê²°ê³¼")
    if st.button("AI ê²€í†  ì‹¤í–‰"):
    if not api_key:
        st.error("OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        doc_json = st.session_state.get("doc_json")   # ì„¸ì…˜ì—ì„œ ì•ˆì „í•˜ê²Œ êº¼ëƒ„
        if not doc_json:
            st.error("ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
        else:
            with st.spinner("AIê°€ ë¬¸ì„œë¥¼ ê²€í†  ì¤‘..."):
                result = integrated_compare(api_key, doc_json, model="gpt-4o")
            st.session_state["analysis_result"] = result
            st.success("ê²€í†  ì™„ë£Œ âœ…")
            st.write(result)
    
